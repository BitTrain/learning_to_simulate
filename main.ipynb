{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0365d8a7",
   "metadata": {},
   "source": [
    "# Learning to Simulate Complex Physics with Graph Neural Networks\n",
    "### [*Sanchez-Gonzalez et al.*](https://github.com/google-deepmind/deepmind-research/tree/master/learning_to_simulate) *(2020)*\n",
    "### Ported from TensorFlow/Sonnet + Graph Nets to TensorFlow/Keras + TensorFlow GNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a157cc2",
   "metadata": {},
   "source": [
    "## Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "DRIVE_DIR = \"/content/drive/MyDrive/learning_to_simulate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df59a3",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow_gnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f634e76",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e0508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:04:48.992483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.16.2\n",
      "TensorFlow GNN 1.0.3\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging, os, pickle, sys\n",
    "\n",
    "# if os.path.exists(\"learning_to_simulate\"):\n",
    "#     %cd learning_to_simulate\n",
    "#     !git fetch\n",
    "#     !git checkout no-batching\n",
    "#     !git pull origin no-batching\n",
    "# else:\n",
    "#     !git clone -b no-batching https://github.com/BitTrain/learning_to_simulate.git\n",
    "#     %cd learning_to_simulate\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "PARENT_DIR = os.path.dirname(BASE_DIR)\n",
    "if PARENT_DIR not in sys.path:\n",
    "    sys.path.append(PARENT_DIR)\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = '1'  # tensorflow_gnn requires Keras v2\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "from learning_to_simulate import utils, settings\n",
    "from learning_to_simulate.models.learned_simulator import LearnedSimulator\n",
    "settings.TF_DEBUG_MODE = False  # Eager data, input checks\n",
    "\n",
    "print(\"TensorFlow\", tf.__version__)\n",
    "print(\"TensorFlow GNN\", tfgnn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250fc729",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2ea74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters configured:\n",
      "DATASET:               WaterDrop\n",
      "DATA_PATH:             /Users/dinbergare/Desktop/experimental/learning_to_simulate/datasets/deepmind/WaterDrop\n",
      "MODEL_PATH:            /Users/dinbergare/Desktop/experimental/learning_to_simulate/datasets/local/WaterDrop/weights\n",
      "OUTPUT_PATH:           /Users/dinbergare/Desktop/experimental/learning_to_simulate/datasets/local/WaterDrop/rollouts\n",
      "LOG_PATH:              /Users/dinbergare/Desktop/experimental/learning_to_simulate/datasets/local/WaterDrop/logs\n",
      "MODE:                  train\n",
      "BATCH_SIZE:            None\n",
      "EVAL_SPLIT:            test\n",
      "NUM_STEPS:             20000000\n",
      "NOISE_STD:             0.0003\n",
      "VELOCITY_CONTEXT_SIZE: 5\n",
      "NUM_PARTICLE_TYPES:    9\n",
      "STATIC_PARTICLE_ID:    3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Available datasets from Google DeepMind\n",
    "\"\"\"\n",
    "    \"WaterDrop\"\n",
    "    \"Water\"\n",
    "    \"Sand\"\n",
    "    \"Goop\"\n",
    "    \"MultiMaterial\"\n",
    "    \"RandomFloor\"\n",
    "    \"WaterRamps\"\n",
    "    \"SandRamps\"\n",
    "    \"FluidShake\"\n",
    "    \"FluidShakeBox\"\n",
    "    \"Continuous\"\n",
    "    \"WaterDrop-XL\"\n",
    "    \"Water-3D\"\n",
    "    \"Sand-3D\"\n",
    "    \"Goop-3D\"\n",
    "\"\"\"\n",
    "DATASET = \"WaterDrop\"\n",
    "DRIVE_DIR = os.path.join(BASE_DIR, \"datasets\", \"local\")\n",
    "\n",
    "params = {\n",
    "    \"DATASET\": DATASET,\n",
    "    \"DATA_PATH\": os.path.join(BASE_DIR, \"datasets\", \"deepmind\", DATASET),\n",
    "    \"MODEL_PATH\": os.path.join(DRIVE_DIR, DATASET, \"weights\"),\n",
    "    \"OUTPUT_PATH\": os.path.join(DRIVE_DIR, DATASET, \"rollouts\"),\n",
    "    \"LOG_PATH\": os.path.join(DRIVE_DIR, DATASET, \"logs\"),\n",
    "    \"MODE\": \"train\",\n",
    "    \"BATCH_SIZE\": None,  # not supported in this version\n",
    "    \"EVAL_SPLIT\": \"test\",\n",
    "    \"NUM_STEPS\": 20_000_000,  # tunable\n",
    "    \"NOISE_STD\": 3e-4,  # @S-G, p. 6\n",
    "    \"VELOCITY_CONTEXT_SIZE\": 5,  # @S-G, p. 4\n",
    "    \"NUM_PARTICLE_TYPES\": 9,  # hardcoded\n",
    "    \"STATIC_PARTICLE_ID\": 3,  # hardcoded\n",
    "}\n",
    "\n",
    "if not os.path.exists(params[\"DATA_PATH\"]):\n",
    "    print(f\"Dataset '{DATASET}' not found at {params['DATA_PATH']}. Downloading...\")\n",
    "    path_to_script = os.path.join(BASE_DIR, \"download_dataset.sh\")\n",
    "    os.system(f\"bash {path_to_script} {DATASET} {os.path.dirname(params['DATA_PATH'])}\")\n",
    "\n",
    "if not os.path.exists(params[\"MODEL_PATH\"]):\n",
    "    os.makedirs(params[\"MODEL_PATH\"], exist_ok=True)\n",
    "    print(f\"Created model weights path {params['MODEL_PATH']}\")\n",
    "\n",
    "if not os.path.exists(params[\"OUTPUT_PATH\"]):\n",
    "    os.makedirs(params[\"OUTPUT_PATH\"], exist_ok=True)\n",
    "    print(f\"Created rollouts output path {params['OUTPUT_PATH']}\")\n",
    "\n",
    "if not os.path.exists(params[\"LOG_PATH\"]):\n",
    "    os.makedirs(params[\"LOG_PATH\"], exist_ok=True)\n",
    "    print(f\"Created TensorBoard logging path {params['LOG_PATH']}\")\n",
    "\n",
    "print(\"\\nParameters configured:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}:\".ljust(22), f\"{value}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187368c6",
   "metadata": {},
   "source": [
    "## Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed20cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, metadata, timestamp, params):\n",
    "    os.makedirs(params[\"MODEL_PATH\"], exist_ok=True)\n",
    "    window_length = params[\"VELOCITY_CONTEXT_SIZE\"] + 2\n",
    "    train_ds, train_size = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        split=\"train\",\n",
    "        mode=\"one_step_train\",\n",
    "        window_length=window_length,\n",
    "        materialize_cache=False  # Caches all pre-processed examples in memory\n",
    "    )\n",
    "    valid_ds, valid_size = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        split=\"valid\",\n",
    "        mode=\"one_step_train\",\n",
    "        window_length=window_length,\n",
    "        materialize_cache=False\n",
    "    )\n",
    "    test_ds, test_size = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        split=\"test\",\n",
    "        mode=\"one_step\",\n",
    "        window_length=window_length,\n",
    "        materialize_cache=False\n",
    "    )\n",
    "    if all((train_size, valid_size, test_size)):\n",
    "        total_size = train_size + valid_size + test_size\n",
    "        print(f\"\\nDataset summary:\")\n",
    "        print(f\"{'Split':<12} {'Examples':>10} {'Percent':>10}\")\n",
    "        print(\"-\" * 34)\n",
    "        print(f\"{'Train':<12} {train_size:>10,} {train_size / total_size:>9.1%}\")\n",
    "        print(f\"{'Valid':<12} {valid_size:>10,} {valid_size / total_size:>9.1%}\")\n",
    "        print(f\"{'Test':<12} {test_size:>10,} {test_size / total_size:>9.1%}\")\n",
    "        print(\"-\" * 34)\n",
    "        print(f\"{'Total':<12} {total_size:>10,} {100:>9.1f}%\\n\")\n",
    "    try:\n",
    "        for dummy in train_ds.take(1):\n",
    "            inputs = dict(dummy)  # Copy to mutable dict\n",
    "            inputs[\"positions\"] = inputs[\"positions\"][:, :-1, :]  # Remove target\n",
    "            model(inputs)  # Build\n",
    "        checkpoint = utils.io.get_latest_checkpoint(params[\"MODEL_PATH\"])\n",
    "        model.load_weights(checkpoint)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved model weights. Training from scratch.\")\n",
    "    try:\n",
    "        steps_per_epoch = 100  # tunable\n",
    "        model.fit(\n",
    "            train_ds,\n",
    "            epochs=params[\"NUM_STEPS\"] // steps_per_epoch,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=valid_ds,\n",
    "            validation_steps=steps_per_epoch,\n",
    "            validation_freq=10,  # tuned to training : validation ratio\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=os.path.join(params[\"MODEL_PATH\"], f\"{timestamp}.weights.h5\"),\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    save_freq=\"epoch\"\n",
    "                ),\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=10,  # tunable\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                ),\n",
    "                tf.keras.callbacks.LambdaCallback(\n",
    "                    on_epoch_end=lambda epoch, logs: logs.update(\n",
    "                        { \"learning_rate\": tf.keras.backend.get_value(model.optimizer.learning_rate) }\n",
    "                    )\n",
    "                ),\n",
    "                tf.keras.callbacks.TensorBoard(\n",
    "                    log_dir=os.path.join(params[\"LOG_PATH\"], timestamp),\n",
    "                    write_graph=False,\n",
    "                    update_freq=\"epoch\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        metrics = model.evaluate(test_ds, return_dict=True, verbose=1)\n",
    "        logging.info(\"Evaluation metrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            logging.info(f\"{k}: {v:.6f}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        model.save_weights(os.path.join(params[\"MODEL_PATH\"], f\"{timestamp}.crash.weights.h5\"))\n",
    "        print(f\"Weights saved to {params['MODEL_PATH']}.\")\n",
    "        return\n",
    "\n",
    "def run_eval(model, metadata, timestamp, params):\n",
    "    eval_ds, eval_size = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        split=params[\"EVAL_SPLIT\"],\n",
    "        mode=\"one_step\",\n",
    "        window_length=params[\"VELOCITY_CONTEXT_SIZE\"] + 2\n",
    "    )\n",
    "    for dummy in eval_ds.take(1):\n",
    "        inputs = dict(dummy)  # Copy to mutable dict\n",
    "        inputs[\"positions\"] = inputs[\"positions\"][:, :-1, :]  # Remove target\n",
    "        model(inputs)  # Build\n",
    "    checkpoint = utils.io.get_latest_checkpoint(params[\"MODEL_PATH\"])\n",
    "    model.load_weights(checkpoint)\n",
    "    metrics = model.evaluate(eval_ds, return_dict=True)\n",
    "    logging.info(\"Evaluation metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        logging.info(f\"{k}: {v:.6f}\")\n",
    "\n",
    "def run_rollout(model, metadata, timestamp, params):\n",
    "    os.makedirs(params[\"OUTPUT_PATH\"], exist_ok=True)\n",
    "    rollout_ds, rollout_size = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        split=params[\"EVAL_SPLIT\"],\n",
    "        mode=\"rollout\"\n",
    "    )\n",
    "    for dummy in rollout_ds.take(1):\n",
    "        inputs = dict(dummy)  # Copy to mutable dict\n",
    "        num_seed = params[\"VELOCITY_CONTEXT_SIZE\"] + 1\n",
    "        inputs[\"positions\"] = inputs[\"positions\"][:, :num_seed, :]  # Window\n",
    "        model(inputs)  # Build\n",
    "    checkpoint = utils.io.get_latest_checkpoint(params[\"MODEL_PATH\"])\n",
    "    model.load_weights(checkpoint)\n",
    "    num_steps =  metadata[\"sequence_length\"] - params[\"VELOCITY_CONTEXT_SIZE\"]\n",
    "    for i, example in enumerate(rollout_ds, start=1):\n",
    "        result = model.rollout(example, num_steps=num_steps)\n",
    "        result[\"metadata\"] = metadata\n",
    "        filename = os.path.join(params[\"OUTPUT_PATH\"], f\"rollout_{params['EVAL_SPLIT']}_{i}.pkl\")\n",
    "        logging.info(f\"Rollout {i} computed for {num_steps} steps. Saving to {filename}\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabaf767",
   "metadata": {},
   "source": [
    "## Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c141e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    metadata = utils.io.load_metadata(argv[\"DATA_PATH\"])\n",
    "\n",
    "    model = LearnedSimulator(\n",
    "        dim=metadata[\"dim\"],\n",
    "        cutoff_radius=metadata[\"default_connectivity_radius\"],\n",
    "        boundaries=metadata[\"bounds\"],\n",
    "        noise_std=argv[\"NOISE_STD\"],\n",
    "        normalization_stats=utils.io.get_normalization_stats(metadata, argv[\"NOISE_STD\"], argv[\"NOISE_STD\"]),\n",
    "        num_particle_types=argv[\"NUM_PARTICLE_TYPES\"],\n",
    "        static_particle_type_id=argv[\"STATIC_PARTICLE_ID\"],\n",
    "        velocity_context_size=argv[\"VELOCITY_CONTEXT_SIZE\"],\n",
    "        bitwave_sizes=(9, 5, 4, 3, 2, 2, 2, 2, 1, 1, 1),\n",
    "        bitqueue_size=9,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(  # S-G, p. 12\n",
    "                initial_learning_rate=1e-4,\n",
    "                decay_steps=argv[\"NUM_STEPS\"],\n",
    "                decay_rate=1e-2\n",
    "            )  # 1e4 -> 1e6 exponentially over all training steps, can be more aggressive\n",
    "        )\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    if argv[\"MODE\"] == \"train\":\n",
    "        run_train(model, metadata, timestamp, argv)\n",
    "    elif argv[\"MODE\"] == \"eval\":\n",
    "        run_eval(model, metadata, timestamp, argv)\n",
    "    elif argv[\"MODE\"] == \"rollout\":\n",
    "        run_rollout(model, metadata, timestamp, argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf44848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 21:05:18.283914: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:7: Filling up shuffle buffer (this may take a while): 9951 of 10000\n",
      "2025-06-20 21:05:18.286497: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "2025-06-20 21:05:18.287264: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'residual_next_state' (type ResidualNextState).\n\nA ResidualNextState() requires an update_fn whose output has the same shape as the input state, but got output shape [307, 128] vs input shape [307, 256] from single input.\n\nCall arguments received by layer 'residual_next_state' (type ResidualNextState):\n  • inputs=('tf.Tensor(shape=(307, 256), dtype=float32)', {'neighbors': 'tf.Tensor(shape=(307, 128), dtype=float32)'}, {})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mTF_DEBUG_MODE:\n\u001b[1;32m      5\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39menable_debug_mode()\n\u001b[0;32m----> 7\u001b[0m main(params)\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     26\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 29\u001b[0m     run_train(model, metadata, timestamp, argv)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m argv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     31\u001b[0m     run_eval(model, metadata, timestamp, argv)\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m(model, metadata, timestamp, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dummy)  \u001b[38;5;66;03m# Copy to mutable dict\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Remove target\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     model(inputs)  \u001b[38;5;66;03m# Build\u001b[39;00m\n\u001b[1;32m     40\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mget_latest_checkpoint(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/experimental/learning_to_simulate/models/learned_simulator.py:119\u001b[0m, in \u001b[0;36mLearnedSimulator.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    117\u001b[0m acceleration \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros_like(positions[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m    118\u001b[0m input_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_graph_tensor(positions, particle_type, acceleration, global_context)\n\u001b[0;32m--> 119\u001b[0m acceleration, logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gnn(input_graph, acceleration, training)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acceleration, logits\n",
      "File \u001b[0;32m~/Desktop/experimental/learning_to_simulate/models/graph_network.py:127\u001b[0m, in \u001b[0;36mEncodeProcessDecode.call\u001b[0;34m(self, input_graph, acceleration, training)\u001b[0m\n\u001b[1;32m    120\u001b[0m node_set \u001b[38;5;241m=\u001b[39m latent_graph\u001b[38;5;241m.\u001b[39mnode_sets[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_set_name]\n\u001b[1;32m    121\u001b[0m latent_graph \u001b[38;5;241m=\u001b[39m latent_graph\u001b[38;5;241m.\u001b[39mreplace_features(node_sets\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_set_name: {\n\u001b[1;32m    123\u001b[0m         tfgnn\u001b[38;5;241m.\u001b[39mHIDDEN_STATE: node_set[tfgnn\u001b[38;5;241m.\u001b[39mHIDDEN_STATE],\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acc_embed(acceleration),\n\u001b[1;32m    125\u001b[0m     }\n\u001b[1;32m    126\u001b[0m })\n\u001b[0;32m--> 127\u001b[0m latent_graph \u001b[38;5;241m=\u001b[39m processor(latent_graph, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m    128\u001b[0m logits \u001b[38;5;241m=\u001b[39m decoder(latent_graph, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m    129\u001b[0m logits \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(logits, (\u001b[38;5;241m*\u001b[39macceleration\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bitqueue_range))\n",
      "File \u001b[0;32m~/Desktop/experimental/learning_to_simulate/layers/message_passing.py:94\u001b[0m, in \u001b[0;36mCustomVanillaMPNNGraphUpdate.<locals>._Wrapper.call\u001b[0;34m(self, graph, training)\u001b[0m\n\u001b[1;32m     92\u001b[0m         concat \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([hidden, extra], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m         graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mreplace_features(node_sets\u001b[38;5;241m=\u001b[39m{name: {tfgnn\u001b[38;5;241m.\u001b[39mHIDDEN_STATE: concat}})\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m base_layer(graph, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow_gnn/keras/layers/graph_update.py:252\u001b[0m, in \u001b[0;36mGraphUpdate.call\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_set_name, update_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_set_updates\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m    250\u001b[0m   features \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mnode_sets[node_set_name]\u001b[38;5;241m.\u001b[39mget_features_dict()\n\u001b[1;32m    251\u001b[0m   features\u001b[38;5;241m.\u001b[39mupdate(_ensure_dict(\n\u001b[0;32m--> 252\u001b[0m       update_fn(graph, node_set_name\u001b[38;5;241m=\u001b[39mnode_set_name)))\n\u001b[1;32m    253\u001b[0m   node_set_features[node_set_name] \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m    254\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mreplace_features(node_sets\u001b[38;5;241m=\u001b[39mnode_set_features)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow_gnn/keras/layers/graph_update.py:452\u001b[0m, in \u001b[0;36mNodeSetUpdate.call\u001b[0;34m(self, graph, node_set_name)\u001b[0m\n\u001b[1;32m    450\u001b[0m next_state_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(next_state_inputs)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(next_state_inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInternal error\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_state(next_state_inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow_gnn/keras/layers/next_state.py:248\u001b[0m, in \u001b[0;36mResidualNextState.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    244\u001b[0m   tf\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    245\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResidualNextState() called on empty input state (latent node set?); \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill omit residual link.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_connection_feature\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mis_compatible_with(net\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 248\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    249\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA ResidualNextState() requires an update_fn whose \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput has the same shape as the input state, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnet\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskip_connection_feature\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskip_connection_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m   net \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39madd(net, skip_connection_feature)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'residual_next_state' (type ResidualNextState).\n\nA ResidualNextState() requires an update_fn whose output has the same shape as the input state, but got output shape [307, 128] vs input shape [307, 256] from single input.\n\nCall arguments received by layer 'residual_next_state' (type ResidualNextState):\n  • inputs=('tf.Tensor(shape=(307, 256), dtype=float32)', {'neighbors': 'tf.Tensor(shape=(307, 128), dtype=float32)'}, {})"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tf.get_logger().setLevel(logging.ERROR)  # Suppress TF warnings\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    if settings.TF_DEBUG_MODE:\n",
    "        tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "    main(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
