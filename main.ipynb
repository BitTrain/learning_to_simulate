{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0365d8a7",
   "metadata": {},
   "source": [
    "# Learning to Simulate Complex Physics with Graph Neural Networks\n",
    "### [*Sanchez-Gonzalez et al.*](https://github.com/google-deepmind/deepmind-research/tree/master/learning_to_simulate) *(2020)*\n",
    "### Ported from TensorFlow/Sonnet + Graph Nets to TensorFlow/Keras + TensorFlow GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Move into cloud storage, e.g. My Drive\n",
    "%cd /content/drive/MyDrive\n",
    "\n",
    "import os\n",
    "if os.path.exists(\"learning_to_simulate\"):\n",
    "    %cd learning_to_simulate\n",
    "    !git pull\n",
    "else:\n",
    "    !git clone https://github.com/BitTrain/learning_to_simulate.git\n",
    "    %cd learning_to_simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df59a3",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow_gnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f634e76",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e0508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:04:27.354349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.16.2\n",
      "TensorFlow GNN 1.0.3\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging, os, pickle, sys\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "PARENT_DIR = os.path.dirname(BASE_DIR)\n",
    "if PARENT_DIR not in sys.path:\n",
    "    sys.path.append(PARENT_DIR)\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = '1'  # tensorflow_gnn requires Keras v2\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "from learning_to_simulate import utils, settings\n",
    "from learning_to_simulate.models.learned_simulator import LearnedSimulator\n",
    "settings.TF_DEBUG_MODE = False  # Eager data, input checks\n",
    "\n",
    "print(\"TensorFlow\", tf.__version__)\n",
    "print(\"TensorFlow GNN\", tfgnn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250fc729",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2ea74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Available datasets from Google DeepMind\n",
    "\"\"\"\n",
    "    \"WaterDrop\"\n",
    "    \"Water\"\n",
    "    \"Sand\"\n",
    "    \"Goop\"\n",
    "    \"MultiMaterial\"\n",
    "    \"RandomFloor\"\n",
    "    \"WaterRamps\"\n",
    "    \"SandRamps\"\n",
    "    \"FluidShake\"\n",
    "    \"FluidShakeBox\"\n",
    "    \"Continuous\"\n",
    "    \"WaterDrop-XL\"\n",
    "    \"Water-3D\"\n",
    "    \"Sand-3D\"\n",
    "    \"Goop-3D\"\n",
    "\"\"\"\n",
    "\n",
    "DATASET = \"WaterDrop\"\n",
    "\n",
    "params = {\n",
    "    \"DATASET\": DATASET,\n",
    "    \"DATA_PATH\": os.path.join(BASE_DIR, \"datasets\", \"deepmind\", DATASET),\n",
    "    \"MODEL_PATH\": os.path.join(BASE_DIR, \"datasets\", \"local\", DATASET, \"weights\"),\n",
    "    \"OUTPUT_PATH\": os.path.join(BASE_DIR, \"datasets\", \"local\", DATASET, \"rollouts\"),\n",
    "    \"MODE\": \"train\",\n",
    "    \"BATCH_SIZE\": 2,  # @S-G, p. 5\n",
    "    \"EVAL_SPLIT\": \"test\",\n",
    "    \"NUM_STEPS\": 20_000_000,  # tunable\n",
    "    \"NOISE_STD\": 3e-4,  # @S-G, p. 6\n",
    "    \"VELOCITY_CONTEXT_SIZE\": 5,  # @S-G, p. 4\n",
    "    \"NUM_PARTICLE_TYPES\": 9,  # hardcoded\n",
    "    \"STATIC_PARTICLE_ID\": 3,  # hardcoded\n",
    "}\n",
    "\n",
    "if not os.path.exists(params[\"DATA_PATH\"]):\n",
    "    print(f\"Dataset '{DATASET}' not found at {params['DATA_PATH']}. Downloading...\")\n",
    "    path_to_script = os.path.join(BASE_DIR, \"download_dataset.sh\")\n",
    "    os.system(f\"bash {path_to_script} {DATASET} {os.path.dirname(params['DATA_PATH'])}\")\n",
    "\n",
    "if not os.path.exists(params[\"MODEL_PATH\"]):\n",
    "    os.makedirs(params[\"MODEL_PATH\"], exist_ok=True)\n",
    "    print(f\"Created model weights path {params['MODEL_PATH']}\")\n",
    "\n",
    "if not os.path.exists(params[\"OUTPUT_PATH\"]):\n",
    "    os.makedirs(params[\"OUTPUT_PATH\"], exist_ok=True)\n",
    "    print(f\"Created rollouts output path {params['OUTPUT_PATH']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187368c6",
   "metadata": {},
   "source": [
    "## Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, metadata, timestamp, params):\n",
    "    os.makedirs(params[\"MODEL_PATH\"], exist_ok=True)\n",
    "    window_length = params[\"VELOCITY_CONTEXT_SIZE\"] + 2\n",
    "    train_ds = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        params[\"BATCH_SIZE\"],\n",
    "        split=\"train\",\n",
    "        mode=\"one_step_train\",\n",
    "        window_length=window_length \n",
    "    )\n",
    "    valid_ds = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        params[\"BATCH_SIZE\"],\n",
    "        split=\"valid\",\n",
    "        mode=\"one_step\",\n",
    "        window_length=window_length\n",
    "    )\n",
    "    test_ds = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        params[\"BATCH_SIZE\"],\n",
    "        split=\"test\",\n",
    "        mode=\"one_step\",\n",
    "        window_length=window_length\n",
    "    )\n",
    "    try:\n",
    "        for dummy in train_ds.take(1):\n",
    "            model(dummy)  # Build\n",
    "        checkpoint = utils.io.get_latest_checkpoint(params[\"MODEL_PATH\"])\n",
    "        model.load_weights(checkpoint)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved model weights. Training from scratch.\")\n",
    "    try:\n",
    "        steps_per_epoch = 100  # tunable\n",
    "        model.fit(\n",
    "            train_ds,\n",
    "            epochs=params[\"NUM_STEPS\"] // steps_per_epoch,\n",
    "            steps_per_epoch=steps_per_epoch,  # save frequency\n",
    "            validation_data=valid_ds,\n",
    "            validation_freq=1,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=os.path.join(params[\"MODEL_PATH\"], f\"{timestamp}.weights.h5\"),\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    save_freq=\"epoch\"\n",
    "                ),\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=10,  # tunable\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                ),\n",
    "                tf.keras.callbacks.LambdaCallback(\n",
    "                    on_epoch_end=lambda epoch, logs: (\n",
    "                        print(f\"lr: {tf.keras.backend.get_value(model.optimizer.learning_rate):.4g}\")\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        metrics = model.evaluate(test_ds, return_dict=True, verbose=1)\n",
    "        logging.info(\"Evaluation metrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            logging.info(f\"{k}: {v:.6f}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        model.save_weights(os.path.join(params[\"MODEL_PATH\"], f\"{timestamp}.crash.weights.h5\"))\n",
    "        print(f\"Weights saved to {params['MODEL_PATH']}.\")\n",
    "        return\n",
    "\n",
    "def run_eval(model, metadata, timestamp, params):\n",
    "    eval_ds = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        params[\"BATCH_SIZE\"],\n",
    "        split=params[\"EVAL_SPLIT\"],\n",
    "        mode=\"one_step\",\n",
    "        window_length=params[\"VELOCITY_CONTEXT_SIZE\"] + 2\n",
    "    )\n",
    "    for dummy in eval_ds.take(1):\n",
    "        model(dummy)  # Build\n",
    "    checkpoint = utils.io.get_latest_checkpoint(params[\"MODEL_PATH\"])\n",
    "    model.load_weights(checkpoint)\n",
    "    metrics = model.evaluate(eval_ds, steps=1, return_dict=True)\n",
    "    logging.info(\"Evaluation metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        logging.info(f\"{k}: {v:.6f}\")\n",
    "\n",
    "def run_rollout(model, metadata, timestamp, params):\n",
    "    os.makedirs(params[\"OUTPUT_PATH\"], exist_ok=True)\n",
    "    rollout_ds = utils.io.load_dataset(\n",
    "        params[\"DATA_PATH\"],\n",
    "        split=params[\"EVAL_SPLIT\"],\n",
    "        mode=\"rollout\"\n",
    "    )\n",
    "    for dummy in rollout_ds.take(1):\n",
    "        model(dummy)  # Build\n",
    "    checkpoint = utils.io.get_latest_checkpoint(params[\"MODEL_PATH\"])\n",
    "    model.load_weights(checkpoint)\n",
    "    num_steps =  metadata[\"sequence_length\"] - params[\"VELOCITY_CONTEXT_SIZE\"]\n",
    "    for i, example in enumerate(rollout_ds.take(2), start=1):\n",
    "        result = model.rollout(example, num_steps=num_steps)\n",
    "        result[\"metadata\"] = metadata\n",
    "        filename = os.path.join(params[\"OUTPUT_PATH\"], f\"rollout_{params['EVAL_SPLIT']}_{i}.pkl\")\n",
    "        logging.info(f\"Rollout {i} computed for {num_steps} steps. Saving to {filename}\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabaf767",
   "metadata": {},
   "source": [
    "## Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c141e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    metadata = utils.io.load_metadata(argv[\"DATA_PATH\"])\n",
    "\n",
    "    model = LearnedSimulator(\n",
    "        dim=metadata[\"dim\"],\n",
    "        cutoff_radius=metadata[\"default_connectivity_radius\"],\n",
    "        boundaries=metadata[\"bounds\"],\n",
    "        noise_std=argv[\"NOISE_STD\"],\n",
    "        normalization_stats=utils.io.get_normalization_stats(metadata, argv[\"NOISE_STD\"], argv[\"NOISE_STD\"]),\n",
    "        num_particle_types=argv[\"NUM_PARTICLE_TYPES\"],\n",
    "        static_particle_type_id=argv[\"STATIC_PARTICLE_ID\"],\n",
    "        velocity_context_size=argv[\"VELOCITY_CONTEXT_SIZE\"]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(  # S-G, p. 12\n",
    "                initial_learning_rate=1e-4,\n",
    "                decay_steps=argv[\"NUM_STEPS\"],\n",
    "                decay_rate=1e-2\n",
    "            )  # 1e4 -> 1e6 exponentially over all training steps, can be more aggressive\n",
    "        )\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    if argv[\"MODE\"] == \"train\":\n",
    "        run_train(model, metadata, timestamp, argv)\n",
    "    elif argv[\"MODE\"] == \"eval\":\n",
    "        run_eval(model, metadata, timestamp, argv)\n",
    "    elif argv[\"MODE\"] == \"rollout\":\n",
    "        run_rollout(model, metadata, timestamp, argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinbergare/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No features found in graph piece: KerasTensor(type_spec=ContextSpec({'features': {}, 'sizes': TensorSpec(shape=(2,), dtype=tf.int64, name=None)}, TensorShape([]), tf.int64, tf.int64, None), description=\"created by layer 'input_1'\"). Falling back on MakeEmptyFeature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:05:10.300941: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'tensorflow_gnn.graph.graph_tensor._ImmutableMapping'>\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'tensorflow_gnn.graph.graph_tensor._ImmutableMapping'>\n",
      "No saved model weights. Training from scratch.\n",
      "Epoch 1/200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:05:12.078618: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 6.2116"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:09:18.763482: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 0.0001\n",
      "100/100 [==============================] - 249s 2s/step - loss: 6.1718 - val_loss: 2.1730e-08\n",
      "Epoch 2/200000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5674"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:13:22.450048: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 0.0001\n",
      "100/100 [==============================] - 243s 2s/step - loss: 0.5673 - val_loss: 2.1151e-08\n",
      "Epoch 3/200000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:17:32.300909: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 9.999e-05\n",
      "100/100 [==============================] - 250s 3s/step - loss: 0.3496 - val_loss: 1.4833e-08\n",
      "Epoch 4/200000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2838"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:22:23.670886: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 9.999e-05\n",
      "100/100 [==============================] - 291s 3s/step - loss: 0.2836 - val_loss: 8.7406e-09\n",
      "Epoch 5/200000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:26:38.744890: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 9.999e-05\n",
      "100/100 [==============================] - 255s 3s/step - loss: 0.2462 - val_loss: 8.8632e-09\n",
      "Epoch 6/200000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:30:59.395316: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 9.999e-05\n",
      "100/100 [==============================] - 260s 3s/step - loss: 0.2418 - val_loss: 1.2079e-08\n",
      "Epoch 7/200000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:35:29.020477: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 9.998e-05\n",
      "100/100 [==============================] - 271s 3s/step - loss: 0.3213 - val_loss: 6.1194e-09\n",
      "Epoch 8/200000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:40:01.581511: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 9.998e-05\n",
      "100/100 [==============================] - 272s 3s/step - loss: 0.2945 - val_loss: 1.1373e-08\n",
      "Epoch 9/200000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2276"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:44:52.649404: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr: 9.998e-05\n",
      "100/100 [==============================] - 292s 3s/step - loss: 0.2275 - val_loss: 5.5668e-09\n",
      "Epoch 10/200000\n",
      " 70/100 [====================>.........] - ETA: 52s - loss: 0.2285"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # tf.get_logger().setLevel(logging.ERROR)  # Suppress tf warnings\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    if settings.TF_DEBUG_MODE:\n",
    "        tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "    main(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
